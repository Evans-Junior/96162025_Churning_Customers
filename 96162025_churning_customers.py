# -*- coding: utf-8 -*-
"""96162025_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qzoHcw-VYJG8VEv5JdJW0CTcy3ykApnZ
"""

from google.colab import drive
import pandas as pd,numpy as np
drive.mount('/content/drive/')

"""**Data Collection**

Get DataSet
"""

df=pd.read_csv('/content/drive/My Drive/AI/Telco-Customer-Churn.csv')
df.head()
# data.shape

"""**Cleaning Data**

Removing all columns with nan greater than 30%
"""

df.dropna(thresh= 0.3 * len(df), axis=1, inplace=True)

"""Get all columns with nans

"""

NullData=df.isnull().sum()

NullData

# checking the datatype of these columns
NullData.dtypes
df[NullData.index].dtypes

"""Working on my non-numeric values"""

alpha_df = df[NullData.index].select_dtypes(exclude='number')
alpha_df.isnull().sum()

"""**Finding all colums which are integers or float**"""

# Converting columns to int and float
int_columns=["SeniorCitizen","tenure"]
float_columns=["MonthlyCharges","TotalCharges"]

# Convert specified columns to integers
df[int_columns] = df[int_columns].astype(int)

# Replace empty strings with NaN in 'Column1'
df[float_columns] = df[float_columns].replace(' ', np.nan)

# Convert the columns to float, handling non-convertible values
for col in float_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Display the data types after conversion
df.dtypes

# Fill missing values with the mean of each column
df = df.fillna(df.median())

"""Get the Numeric Dataset"""

numeric_df = df[NullData.index].select_dtypes(include='number')
numeric_df.isnull().sum()
numeric_df

"""Feature engineering

**Scale numerical features**
"""

from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
df_standardized = numeric_df.copy()

# Due to Future scaling challenges and my best features not influding "SeniorCitizen" I am removing it from the scaled
df_standardized[numeric_df.columns] = standard_scaler.fit_transform(numeric_df[numeric_df.columns])

numeric_scaled_df=df_standardized
numeric_scaled_df

"""Has only alphabets"""

alph_df = df[NullData.index].select_dtypes(exclude='number')
alph_df.isnull().sum()

"""removing unnessary columns"""

alph_df.drop("customerID", axis=1, inplace=True)

# Get columns of object type
object_columns = alph_df.select_dtypes(include='object').columns

# Check for empty strings in each column
columns_with_empty_strings = []
for col in object_columns:
    if any(df[col] == " "):
        columns_with_empty_strings.append(col)

# Display columns with empty strings
if columns_with_empty_strings:
    print("Columns with empty strings:")
    print(columns_with_empty_strings)
else:
    print("No columns have empty strings.")

"""Join my number and aphabet table"""

relevantdata=pd.concat([numeric_scaled_df,alph_df],axis=1)
relevantdata

"""**LabelEcode all aphabet values**"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder

# Encode categorical features
label_encoders = {}
for col in alph_df.columns:
    label_encoders[col] = LabelEncoder()
    relevantdata[col] = label_encoders[col].fit_transform(df[col])

"""**Splitting my dataset to get my X and y**"""

# Split the dataset into features (X) and target (y)
X = relevantdata.drop('Churn', axis=1)  # Features
y = relevantdata['Churn']  # Target variable
y

"""**Perform future engineering on my dataset to get top 15 features**"""

# Train Random Forest Classifier
rf = RandomForestClassifier()
rf.fit(X, y)

# Retrieve feature importances
importances = rf.feature_importances_
feature_importances = pd.Series(importances, index=X.columns)
top_features = feature_importances.nlargest(15)  # Get top 15 features
top_features

"""**Visualization of my selected feature**"""

import matplotlib.pyplot as plt

# Assuming 'top_features' contains the top feature importances

# Plotting top feature importances
plt.figure(figsize=(8, 6))
plt.barh(top_features.index, top_features, color='skyblue')
plt.xlabel('Feature Importance')
plt.title('Top 15 Feature Importances from Random Forest Classifier')
plt.show()

import seaborn as sns

# Assuming 'top_features' contains the top feature importances

# Sort the features by their importance values
top_features = top_features.sort_values(ascending=False)

# Create a bar plot using seaborn
plt.figure(figsize=(8, 6))
sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')
plt.xlabel('Feature Importance')
plt.title('Top 15 Feature Importances from Random Forest Classifier')
plt.show()

"""**Feature Selection/Extraction**"""

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

# Create a Random Forest model
rf = RandomForestClassifier()

# Fit the model on your data
rf.fit(X, y)

# Use SelectFromModel to select features based on their importance
sfm = SelectFromModel(rf, threshold='median')

# Assuming 'top_features' is the top 12 important features obtained
selected_features = top_features.index.tolist()

# Use the top features for feature selection
X_selected = X[selected_features]

# Fit the model with selected features
rf_selected = RandomForestClassifier()
rf_selected.fit(X_selected, y)

# Get the indices of selected features
selected_feature_indices = sfm.get_support()

# Map indices to feature names
selected_features = X.columns[selected_feature_indices]

# Display the selected features
selected_features

"""**Get the scaler object to use for the app**"""

from sklearn.preprocessing import StandardScaler

standard_scaler = StandardScaler()
numeric_df_scaled = numeric_df.copy()

# Scaling only selected columns
relevantdata[['tenure', 'MonthlyCharges', 'TotalCharges']] = standard_scaler.fit_transform(numeric_df[['tenure', 'MonthlyCharges', 'TotalCharges']])
alpha_df

pd.set_option('display.max_rows', None)

relevantdata[selected_features]
df[selected_features].head()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'selected_features' contains the names of the selected features and 'X' is your feature matrix

# Select data for the selected features
selected_data = X[selected_features]

# Convert the selected data to a pandas DataFrame for visualization
selected_df = pd.DataFrame(selected_data, columns=selected_features)

# Create a heatmap of the selected features
plt.figure(figsize=(10, 8))
sns.heatmap(selected_df.corr(), annot=True, cmap='coolwarm')
plt.title('Heatmap of Selected Features')
plt.show()

"""Selecting my x and y columns"""

X = relevantdata[selected_features]  # Features
y = relevantdata['Churn']  # Target variable

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV,StratifiedKFold

# Split the data into training and testing sets
# First, split the data into training (80%) and the rest (20%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Then, split the remaining 20% into 50% for validation and 50% for testing
test_percentage = 0.5  # Splitting the remaining 20% into half for test and validation

X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=test_percentage, random_state=42)

""" Train a Multi-Layer Perceptron model"""

!pip install keras==2.12.0
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV
from keras.models import Model
from keras.layers import Input, Dense
from sklearn.model_selection import StratifiedKFold
from keras.callbacks import History

# Define the callback to store training history
history = History()

# Function to create model using Keras functional API
def create_model():
    inputs = Input(shape=(X_train.shape[1],))
    firstlayer = Dense(24, activation='relu')(inputs)
    secondlayer = Dense(12, activation='relu')(firstlayer)
    thirdlayer = Dense(8, activation='tanh')(secondlayer)
    output = Dense(1, activation='sigmoid')(thirdlayer)

    model = Model(inputs=inputs, outputs=output)
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Create KerasClassifier
model = KerasClassifier(build_fn=create_model, epochs=70, batch_size=30, verbose=0)

# Define the grid search parameters
param_grid = {
    'batch_size': [10, 20, 40],
    'epochs': [70,80,90,100],
}

# Perform stratified K-Fold cross-validation
kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Create GridSearchCV
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)
grid_result = grid.fit(X_train, y_train, epochs=90, batch_size=32,validation_data=(X_val, y_val),callbacks=[history])

# Access the best estimator
best_model = grid_result.best_estimator_.model

# Get the training history
training_loss = best_model.history.history['loss']
training_accuracy = best_model.history.history['accuracy']
val_loss = best_model.history.history['val_loss']
val_accuracy = best_model.history.history['val_accuracy']

# Summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
print("Training Loss:", training_loss)
print("Training Accuracy:", training_accuracy)
print("Validation Loss:", val_loss)
print("Validation Accuracy:", val_accuracy)

loss, accuracy=best_model.evaluate(X_train, y_train)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

from sklearn.metrics import accuracy_score, roc_auc_score

# Evaluate the model's accuracy and AUC score
best_model = grid_result.best_estimator_

# Predict on test data
y_pred = best_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Calculate AUC score
auc_score = roc_auc_score(y_test, y_pred)
print(f"AUC Score: {auc_score}")

# Retrain the model on the full training dataset with the best parameters found
best_params = grid_result.best_params_
model = KerasClassifier(build_fn=create_model, **best_params)
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=90, batch_size=40, verbose=0)

# Accessing the training history
history_dict = history.history

# Evaluating model accuracy and loss on test data
test_loss, test_accuracy = model.model.evaluate(X_test, y_test)

# Predict on test data
y_pred = model.predict(X_test)
y_pred_val = model.predict(X_val)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
accuracy_val = accuracy_score(y_val, y_pred_val)
print(f"Test Accuracy: {accuracy}")
print(f"Validation Accuracy: {accuracy_val}")

# Calculate AUC score
auc_score = roc_auc_score(y_test, y_pred)
auc_score_val = roc_auc_score(y_val, y_pred_val)
print(f"Test AUC Score: {auc_score}")
print(f"Validation AUC Score: {auc_score_val}")

"""**Saving Model **"""

import pickle
with open('util.pkl', 'wb') as file:
    pickle.dump(standard_scaler, file)

# Save the trained model to a file
model.model.save('my_trained_keras_model.h5')